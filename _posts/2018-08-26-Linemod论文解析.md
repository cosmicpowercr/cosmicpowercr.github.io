

# Linemod论文解析

## 1 背景
&ensp;因为暑假要做一款Bin_picking的机器人，同学们向我提出要计算出物体的六轴位姿以便于抓取。<br>
经过种种摸爬滚打之后（自我创造），还是被这个方法的实时性与算法简便性所折服，所以我打算采用这一篇论文进行复现。<br>
&ensp;linemod一共有两个版本，第一版是没用并行运算进行加速的版本，但已经能做到实时了，第二个版本是介绍如何将该算法改为并行运算的版本，并增加了计算时间的章节。

>[1]Multimodal Templates for Real-Time Detection of Texture-less Objects in
Heavily Cluttered Scenes<br>
[2]Gradient Response Maps for Real-Time Detection of Textureless Objects



## 2 效果
 >Our new approach runs in real time and can parse a 640  480 image with over 3,000 templates at about 10 fps. 
 >campar.in.tum.de/files/publications/hinterstoisser2012accv.video.avi
 
ps：这些效果图与论文可以在 大牛的主页中找到http://campar.in.tum.de/Main/StefanHinterstoisser

现在我针对第二个并行加速版本进行解析:
## 3 流程图
虽然论文没有详细的给出一个完整的算法流程图，但为了清晰脉络我给出了算法的流程图：

## 4 公式推导
### 4.1 图片rgb通道上的分析
**最核心的推导是下面两个公式的转换**：
$$\varepsilon (\mathcal{I},\mathcal{T},c )=\sum_{r\in p}\Big ( \max \limits_{t\in \mathcal{R}(c+r)}cos|(ori(\mathcal{O},r)-ori(\mathcal{I},t))| \Big )$$
$$\varepsilon (\mathcal{I},\mathcal{T},c )=\sum_{r\in p}S_{ori(\mathcal{O},r)(c+r)}$$
（因为篇幅有限，两个公式的符号意义需要看论文。）
从公式一转变为公式二，从公式长度上就看到了计算量的减少。哪具体是怎么样减少的呢？

&ensp;**首先**作者使用了技术一：spreadding the Orientation的思路将领域内的求最值问题转变为对单一像素求最值问题。而且比较特别地将这项技术转变为位或运算。
&ensp;**此外**由于作者使用的是特定的几个方向的图像梯度，这样就能使用预计算图。即不管模板是怎么样的，我先将输入进来的图片进行梯度的计算与最值的计算。
最后通过作者的数据结构方式，将公式一：对每个邻域计算最大值的问题变为了，从预运算图中查找值并累加的过程。

&ensp;不仅如此，在第二版的论文中作者加入了并性计算的思路。从编程的角度，我们可以看到公式二中有两个循环，一个是对$r$值循环，另外一个是对$c$值循环。而作者使用了一种特殊的数据结构，将$c$值循环转变成了并行的加法运算。

下面我用一个实际的例子，剖析Linemod到底是怎么进行计算的：
图片；
### 4.2 图片深度通道上的分析
&ensp;按照常识来说信息越多效果越好，在模板匹配上也是这样的作者说他使用了深度图上的信息后整个算法的效果提高了。
在深度图上，不是计算梯度而是计法向量，即通过深度图与相机的内计算出来的点云上，对附近的两个三维点$x_{1}$,$x_{2}$所构成的两个向量进行叉积的计算估算出x处的法向量。
&ensp;进而使用处理rgb通道的方法对深度图进行处理，得到深度通道的相似度。

## 后记
难能可贵的是linemod可以在opencv上找到源码https://github.com/opencv/opencv_contrib/blob/master/modules/rgbd/src/linemod.cpp
下一篇博客将会分析源码是怎么样运作的。





